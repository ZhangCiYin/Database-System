{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQcVMx8OCGPqzDNpgPcZpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhangCiYin/Database-System/blob/main/1_Python_Basics_with_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## numpy ##\n",
        "\n",
        "Numpy is the main package for scientific computing in Python.\n"
      ],
      "metadata": {
        "id": "3b2FFjgh6T26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "djj-KlznHcXq"
      },
      "outputs": [],
      "source": [
        "import numpy as np #ÂØºÂÖ•numpyÂ∫ì\n",
        "import time #ÂØºÂÖ•Êó∂Èó¥Â∫ì"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000) #ÈÄöËøároundÈöèÊú∫ÂæóÂà∞‰∏§‰∏™‰∏ÄÁôæ‰∏áÁª¥Â∫¶ÁöÑÊï∞ÁªÑ"
      ],
      "metadata": {
        "id": "oHnEFS2e6GLW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ÂêëÈáèÂåñÁöÑÁâàÊú¨\n",
        "tic = time.time() #Áé∞Âú®ÊµãÈáè‰∏Ä‰∏ãÂΩìÂâçÊó∂Èó¥\n",
        "c = np.dot(a,b)\n",
        "toc = time.time()\n",
        "print(c)\n",
        "print(\"Vectorized version:\" + str(1000*(toc-tic)) + \"ms\") #ÊâìÂç∞‰∏Ä‰∏ãÂêëÈáèÂåñÁöÑÁâàÊú¨ÁöÑÊó∂Èó¥"
      ],
      "metadata": {
        "id": "535A0vFPHf9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69860374-b9dc-49c5-c92c-70dc00181daf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250261.44328897464\n",
            "Vectorized version:8.061647415161133ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ÁªßÁª≠Â¢ûÂä†ÈùûÂêëÈáèÂåñÁöÑÁâàÊú¨\n",
        "c = 0\n",
        "tic = time.time()\n",
        "for i in range(1000000):\n",
        "    c += a[i]*b[i]\n",
        "toc = time.time()\n",
        "print(c)\n",
        "print(\"For loop:\" + str(1000*(toc-tic)) + \"ms\")#ÊâìÂç∞forÂæ™ÁéØÁöÑÁâàÊú¨ÁöÑÊó∂Èó¥"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04OMqeE1ZFLX",
        "outputId": "6af465af-fe74-4dfb-dfe0-9ae10a35d296"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250261.44328897333\n",
            "For loop:473.03271293640137ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- numpy functions\n",
        "- numpy matrix/vector operations\n",
        "- python broadcasting\n",
        "- vectorize code"
      ],
      "metadata": {
        "id": "tsgKa7D2Mxld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='ex-2'></a>\n",
        "### basic_sigmoid\n",
        "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is the logistic function / non-linear function used in Machine Learning (Logistic Regression) / Deep Learning.\n",
        "\n",
        "$ x = (x_1, x_2, ..., x_n)$\n",
        "\n",
        "`math.exp(x)` : Ëº∏ÂÖ•ÂØ¶Êï∏ÔºåÂú® deep learning Â∞ëÁî®ÔºåËº∏ÂÖ•ÂêëÈáèÊúÉÂá∫ÈåØ\n",
        "\n",
        "`np.exp(x)` : Ëº∏ÂÖ• matrices Âíå vectorsÔºåÊìç‰ΩúÊúÉÂ∞çÊï¥ÂÄãÂêëÈáèÔºåapply the exponential function to every element of x. output : `np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})`."
      ],
      "metadata": {
        "id": "iz3rdDAUQOLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute sigmoid of x.\n",
        "    x -- A scalar or numpy array of any size\n",
        "    \"\"\"\n",
        "    # math.exp():Ëº∏ÂÖ•ÂØ¶Êï∏ÔºåÂú®deep learningÂ∞ëÁî®ÔºåËº∏ÂÖ•ÂêëÈáèÊúÉÂá∫ÈåØ\n",
        "    # s = 1 / (1 + math.exp(-x))\n",
        "\n",
        "    # numpy:Ëº∏ÂÖ•matricesÂíåvectors\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "XjtPkP2uN99F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_x = np.array([1, 2, 3])\n",
        "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs3cUWkiSJec",
        "outputId": "d2f085cc-4b2a-4eab-b6db-62b1b7841ae0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-2'></a>\n",
        "### Sigmoid Gradient/sigmoid_derivative\n",
        "compute the gradient of the sigmoid function to optimize loss functions using backpropagation.\n",
        "\n",
        "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
        "\n",
        "1. Set s to be the sigmoid of x. sigmoid(x) function.\n",
        "2. Compute $\\sigma'(x) = s(1-s)$"
      ],
      "metadata": {
        "id": "c9ZSzAMgWFJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Compute the gradient (derivative) of the sigmoid function with respect to its input x.\n",
        "    x -- A scalar or numpy array\n",
        "    \"\"\"\n",
        "    s = sigmoid(x)\n",
        "    ds = ùë† * (1 - ùë†)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "L-yAy3klYffW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_x = np.array([1, 2, 3])\n",
        "print (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxmRd9z6ZCsF",
        "outputId": "113d931f-3b25-4a8f-f2e3-485305f075a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-3'></a>\n",
        "### Reshaping arrays ###\n",
        "\n",
        "[np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)\n",
        "\n",
        "[np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html)\n",
        "- X.shape get the shape (dimension) of a matrix/vector X.\n",
        "- X.reshape(...) reshape X into some other dimension.\n",
        "\n",
        "image : 3D array of shape $(length, height, depth = 3)$.\n",
        "\n",
        "read image : reshape the 3D array into a 1D vector $(length*height*3, 1)$"
      ],
      "metadata": {
        "id": "EDKtjnbTeYA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`image2vector()`\n",
        "\n",
        "takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1).\n",
        "``` python\n",
        "v = v.reshape((v.shape[0] * v.shape[1] * v.shape[2]), 1)\n",
        "```"
      ],
      "metadata": {
        "id": "IMxw1yLB8LhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image2vector(image):\n",
        "    \"\"\"\n",
        "    image -- a numpy array of shape (length, height, depth)\n",
        "    v -- a vector of shape (length*height*depth, 1)\n",
        "    \"\"\"\n",
        "    v = image.reshape((image.shape[0] * image.shape[1] * image.shape[2]), 1)\n",
        "\n",
        "    return v"
      ],
      "metadata": {
        "id": "bfwna_Oqf4Qk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
        "                     [ 0.90714982,  0.52835647],\n",
        "                     [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "                   [[ 0.92814219,  0.96677647],\n",
        "                    [ 0.85304703,  0.52351845],\n",
        "                    [ 0.19981397,  0.27417313]],\n",
        "\n",
        "                   [[ 0.60659855,  0.00533165],\n",
        "                    [ 0.10820313,  0.49978937],\n",
        "                    [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print (\"image2vector(image) = \" + str(image2vector(t_image)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBSUYTGNghCB",
        "outputId": "c0f04a4b-28d1-44eb-e5ce-7eb8034a4b40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image2vector(image) = [[0.67826139]\n",
            " [0.29380381]\n",
            " [0.90714982]\n",
            " [0.52835647]\n",
            " [0.4215251 ]\n",
            " [0.45017551]\n",
            " [0.92814219]\n",
            " [0.96677647]\n",
            " [0.85304703]\n",
            " [0.52351845]\n",
            " [0.19981397]\n",
            " [0.27417313]\n",
            " [0.60659855]\n",
            " [0.00533165]\n",
            " [0.10820313]\n",
            " [0.49978937]\n",
            " [0.34144279]\n",
            " [0.94630077]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1-4'></a>\n",
        "### Normalizing rows\n",
        "\n",
        "Machine Learning and Deep Learning normalize data\n",
        "\n",
        "- better performance\n",
        "\n",
        "- gradient descent converges faster after normalization.\n",
        "\n",
        "normalization changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
        "\n",
        "if\n",
        "$$x = \\begin{bmatrix}\n",
        "        0 & 3 & 4 \\\\\n",
        "        2 & 6 & 4 \\\\\n",
        "\\end{bmatrix}\\tag{3}$$\n",
        "then\n",
        "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
        "    5 \\\\\n",
        "    \\sqrt{56} \\\\\n",
        "\\end{bmatrix}\\tag{4} $$\n",
        "broadcasting\n",
        "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
        "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
        "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
        "\\end{bmatrix}\\tag{5}$$\n",
        "\n",
        "`keepdims=True` the result will broadcast correctly against the original x.\n",
        "\n",
        "`axis=1` norm in a row-wise manner.\n",
        "\n",
        "`axis=0` norm in a column-wise manner.\n",
        "\n",
        "[numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)\n",
        "- parameter `ord` specify the type of normalization to be done (ex: 2-norm)."
      ],
      "metadata": {
        "id": "XdXMR2_WgmOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_rows(x):\n",
        "    \"\"\"\n",
        "    x -- A numpy matrix of shape (n, m)\n",
        "    \"\"\"\n",
        "    x_norm = np.linalg.norm(x, ord = 2, axis=1, keepdims=True)\n",
        "    x = x / x_norm\n",
        "    return x"
      ],
      "metadata": {
        "id": "XoGbzs7hiNH9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[0., 3., 4.],[1., 6., 4.]])\n",
        "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_tw553zgeJs",
        "outputId": "1de4b8b2-1e8f-41bb-bc32-de226c2115cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='ex-7'></a>\n",
        "### softmax (normalizing function) function numpy\n",
        "softmax as a normalizing function used when needs to classify two or more classes.\n",
        "\n",
        "**Instructions**:\n",
        "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
        "\n",
        "\\begin{align*}\n",
        " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
        "    x_1  &&\n",
        "    x_2 &&\n",
        "    ...  &&\n",
        "    x_n  \n",
        "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
        "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
        "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
        "    ...  &&\n",
        "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}}\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  \n",
        "\n",
        "\\begin{align*}\n",
        "softmax(x) &= softmax\\begin{bmatrix}\n",
        "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
        "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
        "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
        "            \\end{bmatrix} \\\\ \\\\&=\n",
        " \\begin{bmatrix}\n",
        "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
        "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
        "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
        "    softmax\\text{(first row of x)}  \\\\\n",
        "    softmax\\text{(second row of x)} \\\\\n",
        "    \\vdots  \\\\\n",
        "    softmax\\text{(last row of x)} \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "rbLd6QTjirmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"m\"\n",
        "- the \"number of training examples\"\n",
        "\n",
        "Softmax performed for all features of each training example, so softmax would be performed on the columns.\n",
        "\n",
        "$m \\times n$  \n",
        "\n",
        "$m$ is the number of rows\n",
        "\n",
        "$n$ is the number of columns."
      ],
      "metadata": {
        "id": "DWFmcy3u81tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    x -- A numpy matrix of shape (m,n)\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply exp() element-wise to x. Use np.exp(...).\n",
        "    x_exp = np.exp(x)\n",
        "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
        "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
        "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
        "    s = x_exp / x_sum\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "xypaHa7YiX5n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_x = np.array([[9, 2, 5, 0, 0],\n",
        "                [7, 5, 0, 0 ,0]])\n",
        "print(\"softmax(x) = \" + str(softmax(t_x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyYtyqkkTz5",
        "outputId": "396ca310-8869-4ff7-f98b-d8c7fc4596e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
            "  1.21052389e-04]\n",
            " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
            "  8.01252314e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## Vectorization\n",
        "\n",
        "deep learning deal with very large datasets\n",
        "\n",
        "use vectorization much cleaner and more efficient.\n",
        "\n",
        "`np.dot()` matrix-matrix or matrix-vector multiplication different from `np.multiply()` and the `*` operator (element-wise multiplication)\n"
      ],
      "metadata": {
        "id": "-_KchGWVkin4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "dot = np.dot(x1,x2)\n",
        "\n",
        "outer = np.outer(x1,x2)\n",
        "\n",
        "mul = np.multiply(x1,x2)"
      ],
      "metadata": {
        "id": "9C1G_X8JkVRZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='ex-8'></a>\n",
        "### L1 loss functions\n",
        "numpy vectorized version of L1 loss.\n",
        "\n",
        "abs(x)\n",
        "\n",
        "loss evaluate the performance of model\n",
        "\n",
        "bigger loss = more different your predictions ($ \\hat{y} $) from the true values ($y$).\n",
        "\n",
        "deep learning use optimization algorithms(Gradient Descent) to train model minimize the cost.\n",
        "\n",
        "L1 loss :\n",
        "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
      ],
      "metadata": {
        "id": "-ZYRGp8smdaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L1(yhat, y):\n",
        "    \"\"\"\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    loss -- the value of the L1 loss function defined above\n",
        "    \"\"\"\n",
        "    loss = np.sum(abs(y - yhat))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "41u8aab_4xNP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L1 = \" + str(L1(yhat, y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZB9l2TR40Ee",
        "outputId": "388ec0e5-7bca-42f6-9464-f8a308c63d97"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1 = 1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='ex-9'></a>\n",
        "### L2 loss functions\n",
        "numpy vectorized L2 loss.\n",
        "\n",
        "np.dot()\n",
        "\n",
        "if\n",
        "\n",
        "$x = [x_1, x_2, ..., x_n]$\n",
        "\n",
        "then\n",
        "\n",
        "`np.dot(x,x)` = $\\sum_{j=1}^n x_j^{2}$.\n",
        "\n",
        "\n",
        "L2 loss $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
      ],
      "metadata": {
        "id": "hP7w2LTH43TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L2(yhat, y):\n",
        "    \"\"\"\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "    loss -- the value of the L2 loss function defined above\n",
        "    \"\"\"\n",
        "\n",
        "    loss = np.dot(y - yhat,y - yhat)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "JrUZEqnG5eUW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "\n",
        "print(\"L2 = \" + str(L2(yhat, y)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q37aCFuD5gCY",
        "outputId": "3043f02f-0d38-4863-e14e-8eb01d5ea9ef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 = 0.43\n"
          ]
        }
      ]
    }
  ]
}